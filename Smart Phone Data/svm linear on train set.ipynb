{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from statistics import mean \n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score,accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "both_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "#print('Shape both:\\t{}\\n'.format(both_df.shape))\n",
    "\n",
    "def get_train_test_data(a):\n",
    "    tr_df=both_df[(both_df.subject != a[0])&(both_df.subject != a[1])\n",
    "                     &(both_df.subject != a[2])&(both_df.subject != a[3])\n",
    "                     &(both_df.subject != a[4])]\n",
    "    \n",
    "    te_df=both_df.loc[both_df['subject'].isin([a[0],a[1],a[2],a[3],a[4]])]\n",
    "                                           \n",
    "    return tr_df,te_df\n",
    "                     \n",
    "\n",
    "def get_label(t):\n",
    "    return t.Activity.values    \n",
    "    \n",
    "def drop_label(t):\n",
    "    return t.drop('Activity' , axis=1).values\n",
    "\n",
    "def encode_label(e):\n",
    "    encoder.fit(e)\n",
    "    return encoder.transform(e)                                          \n",
    "\n",
    "    \n",
    "#print('Shape Train:\\t{}'.format(train_df.shape))\n",
    "#print('Shape Test:\\t{}\\n'.format(test_df.shape))\n",
    "\n",
    "\n",
    "# encoding train test labels\n",
    "\n",
    "#y_score = classifier.predict_proba(test_data_without_label)\n",
    "                                              \n",
    "                                        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    #This function prints and plots the confusion matrix.\n",
    "    #Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "def get_acc_recall_prec_f1_score_and_confusion_mat(a):\n",
    "    \n",
    "    classifier = OneVsRestClassifier(svm.SVC(kernel='linear',\n",
    "                                         probability=True,\n",
    "                                         random_state=0))\n",
    "\n",
    "    train,test=get_train_test_data(a)\n",
    "    \n",
    "    train_label=get_label(train)\n",
    "    test_label=get_label(test)\n",
    "    \n",
    "    train_data_without_label=drop_label(train)\n",
    "    test_data_without_label=drop_label(test)\n",
    "    \n",
    "    encoded_label_of_train_data=encode_label(train_label)\n",
    "    classifier.fit(train_data_without_label, encoded_label_of_train_data)\n",
    "    \n",
    "    \n",
    "    encoded_label_of_test_data=encode_label(test_label)\n",
    "    y_te_pred = classifier.predict(train_data_without_label)\n",
    "    \n",
    "    acc1 = accuracy_score(encoded_label_of_train_data, y_te_pred)\n",
    "    prec1 = precision_score(encoded_label_of_train_data, y_te_pred, average=\"macro\")\n",
    "    rec1 = recall_score(encoded_label_of_train_data, y_te_pred, average=\"macro\")\n",
    "    F1_score1= f1_score(encoded_label_of_train_data, y_te_pred, average=\"macro\")\n",
    "    \n",
    "    print(\"Accuracy: %3.5f, Precision: %3.5f, Recall: %3.5f, f1_score: %3.5f\" % (acc1, prec1, rec1,F1_score1))\n",
    "    \n",
    "    #cfs = confusion_matrix(encoded_label_of_test_data, y_te_pred)\n",
    "    \n",
    "    #plt.figure()\n",
    "    #class_names = encoder.classes_\n",
    "    #plot_confusion_matrix (cfs, classes=class_names,\n",
    "                         # title='SVM Confusion Matrix, without normalization')\n",
    "    \n",
    "    return acc1,prec1,rec1,F1_score1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persons that are kept for testing:\t[1, 2, 3, 4, 5]\n",
      "\n",
      "Accuracy: 0.99344, Precision: 0.99398, Recall: 0.99401, f1_score: 0.99399\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5]\n",
    "print('persons that are kept for testing:\\t{}\\n'.format(a))\n",
    "acc1,prec1,rec1,f1=get_acc_recall_prec_f1_score_and_confusion_mat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persons that are kept for testing:\t[6, 7, 8, 9, 10]\n",
      "\n",
      "Accuracy: 0.99500, Precision: 0.99538, Recall: 0.99538, f1_score: 0.99538\n"
     ]
    }
   ],
   "source": [
    "a=[6,7,8,9,10]\n",
    "print('persons that are kept for testing:\\t{}\\n'.format(a))\n",
    "\n",
    "acc2,prec2,rec2,f2=get_acc_recall_prec_f1_score_and_confusion_mat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persons that are kept for testing:\t[11, 12, 13, 14, 15]\n",
      "\n",
      "Accuracy: 0.99136, Precision: 0.99202, Recall: 0.99202, f1_score: 0.99202\n"
     ]
    }
   ],
   "source": [
    "a=[11,12,13,14,15]\n",
    "print('persons that are kept for testing:\\t{}\\n'.format(a))\n",
    "\n",
    "acc3,prec3,rec3,f3=get_acc_recall_prec_f1_score_and_confusion_mat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persons that are kept for testing:\t[16, 17, 18, 19, 20]\n",
      "\n",
      "Accuracy: 0.99258, Precision: 0.99297, Recall: 0.99293, f1_score: 0.99295\n"
     ]
    }
   ],
   "source": [
    "a=[16,17,18,19,20]\n",
    "print('persons that are kept for testing:\\t{}\\n'.format(a))\n",
    "\n",
    "acc4,prec4,rec4,f4=get_acc_recall_prec_f1_score_and_confusion_mat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persons that are kept for testing:\t[21, 22, 23, 24, 25]\n",
      "\n",
      "Accuracy: 0.99167, Precision: 0.99217, Recall: 0.99210, f1_score: 0.99214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=[21,22,23,24,25]\n",
    "print('persons that are kept for testing:\\t{}\\n'.format(a))\n",
    "\n",
    "acc5,prec5,rec5,f5=get_acc_recall_prec_f1_score_and_confusion_mat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persons that are kept for testing:\t[26, 27, 28, 29, 30]\n",
      "\n",
      "Accuracy: 0.99145, Precision: 0.99195, Recall: 0.99194, f1_score: 0.99194\n"
     ]
    }
   ],
   "source": [
    "a=[26,27,28,29,30]\n",
    "print('persons that are kept for testing:\\t{}\\n'.format(a))\n",
    "\n",
    "acc6,prec6,rec6,f6=get_acc_recall_prec_f1_score_and_confusion_mat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:\t0.9925848835900138\n",
      "\n",
      "Average precision score:\t0.993077699341337\n",
      "\n",
      "Average recall score:\t0.9930627679129566\n",
      "\n",
      "Average F1 score:\t0.9930693460554032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy=[acc1,acc2,acc3,acc4,acc5,acc6]\n",
    "precession=[prec1,prec2,prec3,prec4,prec5,prec6]\n",
    "recall=[rec1,rec2,rec3,rec4,rec5,rec6]\n",
    "f1_score=[f1,f2,f3,f4,f5,f6]\n",
    "\n",
    "print('Average accuracy:\\t{}\\n'.format(mean(accuracy)))\n",
    "print('Average precision score:\\t{}\\n'.format(mean(precession)))\n",
    "print('Average recall score:\\t{}\\n'.format(mean(recall)))\n",
    "print('Average F1 score:\\t{}\\n'.format(mean(f1_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
